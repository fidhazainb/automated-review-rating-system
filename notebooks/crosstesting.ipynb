{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80de42fe-d62b-44df-9c26-ad3697f6fb42",
   "metadata": {},
   "source": [
    " # crosstesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3fef27-fc4f-485a-923a-c24afd88876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models,vectorizer,test loaded\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "path = r\"C:\\Users\\hp\\Desktop\\automatedreviewratingsystem\\models\"\n",
    "\n",
    "model_A = joblib.load(f\"{path}\\\\model_A.pkl\")\n",
    "vec_A   = joblib.load(f\"{path}\\\\vectorizer_A.pkl\")\n",
    "\n",
    "model_B = joblib.load(f\"{path}\\\\model_B.pkl\")\n",
    "vec_B   = joblib.load(f\"{path}\\\\vectorizer_B.pkl\")\n",
    "X_test_A, y_test_A = joblib.load(f\"{path}\\\\imbalanced_test.pkl\")\n",
    "X_test_B, y_test_B = joblib.load(f\"{path}\\\\balanced_test.pkl\")\n",
    "\n",
    "print(\"models,vectorizer,test loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c514fca-72ca-4ed2-b5ee-99d7d826fa41",
   "metadata": {},
   "source": [
    "#vectorising test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2b0879-4534-4823-a97f-7206b8badf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_A_vec_A = vec_A.transform(X_test_A)\n",
    "X_test_B_vec_B = vec_B.transform(X_test_B)\n",
    "X_test_B_vec_A = vec_A.transform(X_test_B)\n",
    "X_test_A_vec_B = vec_B.transform(X_test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbf84db-bdc6-4b18-a028-41f59baa97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab89f8a9-ad10-4de8-a94c-45d6ffa68f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    return (\n",
    "        round(accuracy_score(y, preds) * 100, 2),       \n",
    "        round(precision_score(y, preds, average='macro', zero_division=0) * 100, 2),\n",
    "        round(recall_score(y, preds, average='macro', zero_division=0) * 100, 2),\n",
    "        round(f1_score(y, preds, average='macro', zero_division=0) * 100, 2),\n",
    "        round(f1_score(y, preds, average='weighted', zero_division=0) * 100, 2)   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840f442e-7742-4001-b7c7-12b2db2172e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results.append([\"Model_A(imbalanced)\", \"Test data (own)\", *evaluate(model_A, X_test_A_vec_A, y_test_A)])\n",
    "results.append([\"Model_B(balanced)\", \"Test data (own)\", *evaluate(model_B, X_test_B_vec_B, y_test_B)])\n",
    "results.append([\"Model_A(imbalanced)\", \"Balanced set (cross)\", *evaluate(model_A, X_test_B_vec_A, y_test_B)])\n",
    "results.append([\"Model_B(balanced)\", \"Imbalanced set (cross)\", *evaluate(model_B, X_test_A_vec_B, y_test_A)])\n",
    "\n",
    "summary = pd.DataFrame(results, columns=[\"Model\", \"Tested On\", \"Accuracy %\", \"Precision(Macro)\", \"Recall(Macro)\", \"F1(Macro)\", \"F1(weighted)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fb2c67-8d3f-436c-9995-556ad3488800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final cross-testing summary table (svm)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Tested On</th>\n",
       "      <th>Accuracy %</th>\n",
       "      <th>Precision(Macro)</th>\n",
       "      <th>Recall(Macro)</th>\n",
       "      <th>F1(Macro)</th>\n",
       "      <th>F1(weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_A(imbalanced)</td>\n",
       "      <td>Test data (own)</td>\n",
       "      <td>55.91</td>\n",
       "      <td>54.97</td>\n",
       "      <td>52.84</td>\n",
       "      <td>53.65</td>\n",
       "      <td>55.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model_B(balanced)</td>\n",
       "      <td>Test data (own)</td>\n",
       "      <td>47.59</td>\n",
       "      <td>47.00</td>\n",
       "      <td>47.59</td>\n",
       "      <td>47.08</td>\n",
       "      <td>47.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model_A(imbalanced)</td>\n",
       "      <td>Balanced set (cross)</td>\n",
       "      <td>45.63</td>\n",
       "      <td>47.19</td>\n",
       "      <td>45.63</td>\n",
       "      <td>44.80</td>\n",
       "      <td>44.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model_B(balanced)</td>\n",
       "      <td>Imbalanced set (cross)</td>\n",
       "      <td>67.82</td>\n",
       "      <td>70.42</td>\n",
       "      <td>70.80</td>\n",
       "      <td>69.58</td>\n",
       "      <td>67.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model               Tested On  Accuracy %  Precision(Macro)  \\\n",
       "0  Model_A(imbalanced)         Test data (own)       55.91             54.97   \n",
       "1    Model_B(balanced)         Test data (own)       47.59             47.00   \n",
       "2  Model_A(imbalanced)    Balanced set (cross)       45.63             47.19   \n",
       "3    Model_B(balanced)  Imbalanced set (cross)       67.82             70.42   \n",
       "\n",
       "   Recall(Macro)  F1(Macro)  F1(weighted)  \n",
       "0          52.84      53.65         55.51  \n",
       "1          47.59      47.08         47.08  \n",
       "2          45.63      44.80         44.80  \n",
       "3          70.80      69.58         67.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nFinal cross-testing summary table (svm)\\n\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a62ef523-6444-4920-9554-6d137ba2bef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Model', 'Tested On', 'Accuracy %', 'Precision(Macro)', 'Recall(Macro)',\n",
      "       'F1(Macro)', 'F1(weighted)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(summary_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdc9255-ef3a-466f-b189-980d4f67ef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models, vectorizers, and test data loaded.\n",
      "                  Model               Tested On  Accuracy %  \\\n",
      "0  Model_A (imbalanced)         Test data (own)       55.36   \n",
      "1    Model_B (balanced)         Test data (own)       47.59   \n",
      "2  Model_A (imbalanced)    Balanced set (cross)       45.69   \n",
      "3    Model_B (balanced)  Imbalanced set (cross)       67.82   \n",
      "\n",
      "   Precision (Macro)  Recall (Macro)  F1 (Macro)  F1 (Weighted)  \n",
      "0              53.40           54.80       53.96          55.39  \n",
      "1              47.00           47.59       47.08          47.08  \n",
      "2              49.70           45.69       44.40          44.40  \n",
      "3              70.42           70.80       69.58          67.74  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "path = r\"C:\\Users\\hp\\Desktop\\automatedreviewratingsystem\\models\"\n",
    "\n",
    "model_A = joblib.load(f\"{path}\\\\fine_tuned_logistic_imbalanced.pkl\")\n",
    "vec_A   = joblib.load(f\"{path}\\\\vectorizer_A.pkl\")\n",
    "\n",
    "model_B = joblib.load(f\"{path}\\\\fine_tuned_svm_balanced.pkl\")\n",
    "vec_B   = joblib.load(f\"{path}\\\\vectorizer_B.pkl\")\n",
    "\n",
    "X_test_A, y_test_A = joblib.load(f\"{path}\\\\imbalanced_test.pkl\")\n",
    "X_test_B, y_test_B = joblib.load(f\"{path}\\\\balanced_test.pkl\")\n",
    "\n",
    "print(\"Models, vectorizers, and test data loaded.\")\n",
    "\n",
    "X_test_A_vec_A = vec_A.transform(X_test_A)\n",
    "X_test_B_vec_B = vec_B.transform(X_test_B)\n",
    "X_test_B_vec_A = vec_A.transform(X_test_B)\n",
    "X_test_A_vec_B = vec_B.transform(X_test_A)\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    return (\n",
    "        round(accuracy_score(y, preds) * 100, 2),       \n",
    "        round(precision_score(y, preds, average='macro', zero_division=0) * 100, 2),\n",
    "        round(recall_score(y, preds, average='macro', zero_division=0) * 100, 2),\n",
    "        round(f1_score(y, preds, average='macro', zero_division=0) * 100, 2),\n",
    "        round(f1_score(y, preds, average='weighted', zero_division=0) * 100, 2)   \n",
    "    )\n",
    "\n",
    "results = []\n",
    "results.append([\"Model_A (imbalanced)\", \"Test data (own)\", *evaluate(model_A, X_test_A_vec_A, y_test_A)])\n",
    "results.append([\"Model_B (balanced)\", \"Test data (own)\", *evaluate(model_B, X_test_B_vec_B, y_test_B)])\n",
    "results.append([\"Model_A (imbalanced)\", \"Balanced set (cross)\", *evaluate(model_A, X_test_B_vec_A, y_test_B)])\n",
    "results.append([\"Model_B (balanced)\", \"Imbalanced set (cross)\", *evaluate(model_B, X_test_A_vec_B, y_test_A)])\n",
    "\n",
    "summary = pd.DataFrame(results, columns=[\"Model\", \"Tested On\", \"Accuracy %\", \"Precision (Macro)\", \"Recall (Macro)\", \"F1 (Macro)\", \"F1 (Weighted)\"])\n",
    "\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2c43c-d017-4adc-ac4f-a40803a60c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
